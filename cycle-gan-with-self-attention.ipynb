{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"CycleGAN is a type of Generative Adversarial Network (GAN) that enables image-to-image translation in an unsupervised manner. This means it can convert images from one domain to another without needing paired examples. Common applications include style transfer, season transfer, and photo enhancement.","metadata":{}},{"cell_type":"markdown","source":"**Why Add Self-Attention to CycleGAN?**\n\n\nSelf-attention mechanisms allow the network to weigh the importance of different regions in the input data, regardless of their position. In the context of image translation:\n\nDetail Preservation: Attention can help preserve details by focusing on relevant features, which is especially beneficial in complex translations like changing facial expressions, altering seasons in landscapes, or converting paintings to photographs.\nGlobal Context: It integrates global contextual information better than convolutions alone, which primarily capture local features.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n\nimport cv2\n\nimport os\nimport glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-14T16:37:05.030687Z","iopub.execute_input":"2024-06-14T16:37:05.031072Z","iopub.status.idle":"2024-06-14T16:37:11.335160Z","shell.execute_reply.started":"2024-06-14T16:37:05.031043Z","shell.execute_reply":"2024-06-14T16:37:11.334379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_HEIGHT = 256\nIMG_WIDTH = 256\nimage_transforms = transforms.Compose([\n    transforms.Resize(int(IMG_HEIGHT * 1.12), Image.BICUBIC),\n    transforms.RandomCrop((IMG_HEIGHT, IMG_WIDTH)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:37:11.336970Z","iopub.execute_input":"2024-06-14T16:37:11.337606Z","iopub.status.idle":"2024-06-14T16:37:11.344816Z","shell.execute_reply.started":"2024-06-14T16:37:11.337572Z","shell.execute_reply":"2024-06-14T16:37:11.343846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, root, mode='train', transform=None):\n        self.root = root\n        self.transform = transform\n        self.mode = mode\n        if self.mode == 'train':\n            self.files_A = sorted(glob.glob(os.path.join(root, 'monet_jpg', '*.*'))[:250])\n            self.files_B = sorted(glob.glob(os.path.join(root, 'photo_jpg', '*.*'))[:250])\n        elif self.mode == 'test':\n            self.files_A = sorted(glob.glob(os.path.join(root, 'monet_jpg', '*.*'))[250:])\n            self.files_B = sorted(glob.glob(os.path.join(root, 'photo_jpg', '*.*'))[250:301])\n\n    def __getitem__(self, index):\n        image_A = Image.open(self.files_A[index % len(self.files_A)])\n        image_B = Image.open(self.files_B[index % len(self.files_B)])\n\n        if self.transform:\n            image_A = self.transform(image_A)\n            image_B = self.transform(image_B)\n\n        return {'image_A': image_A, 'image_B': image_B}\n\n    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:37:11.345977Z","iopub.execute_input":"2024-06-14T16:37:11.346248Z","iopub.status.idle":"2024-06-14T16:37:11.356080Z","shell.execute_reply.started":"2024-06-14T16:37:11.346226Z","shell.execute_reply":"2024-06-14T16:37:11.355204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/gan-getting-started'\n\nfrom torchvision import transforms\n\ntransforms_ = transforms.Compose([\n    transforms.Lambda(lambda x: x.convert('RGB')), \n    transforms.Resize((256, 256)),  \n    transforms.ToTensor(), \n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:08.166920Z","iopub.execute_input":"2024-06-14T16:39:08.167628Z","iopub.status.idle":"2024-06-14T16:39:08.173217Z","shell.execute_reply.started":"2024-06-14T16:39:08.167595Z","shell.execute_reply":"2024-06-14T16:39:08.172317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    CustomDataset(root=data_dir, mode='train', transform=transforms_),\n    batch_size=1,  # Smaller batch size for demonstration\n    shuffle=True,\n    num_workers=2  # Adjust according to your system capabilities\n)\n\n# Create the validation/testing dataset loader\nval_dataloader = DataLoader(\n    CustomDataset(root=data_dir, mode='test', transform=transforms_),\n    batch_size=5,  # Larger batch size for validation\n    shuffle=True,\n    num_workers=2\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:11.999417Z","iopub.execute_input":"2024-06-14T16:39:11.999764Z","iopub.status.idle":"2024-06-14T16:39:12.213110Z","shell.execute_reply.started":"2024-06-14T16:39:11.999737Z","shell.execute_reply":"2024-06-14T16:39:12.212353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding self attention inside residual layers","metadata":{}},{"cell_type":"code","source":"from torch import nn\nclass SelfAttention(nn.Module):\n    def __init__(self, in_dim):\n        super(SelfAttention, self).__init__()\n        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n        self.softmax = nn.Softmax(dim=-1)  # Apply softmax to the last dimension\n\n    def forward(self, x):\n        batch, channels, height, width = x.size()\n        query = self.query_conv(x).view(batch, -1, height * width).permute(0, 2, 1)\n        key = self.key_conv(x).view(batch, -1, height * width)\n        value = self.value_conv(x).view(batch, -1, height * width)\n\n        energy = torch.bmm(query, key)  # Batch matrix-matrix product\n        attention = self.softmax(energy)\n        out = torch.bmm(value, attention.permute(0, 2, 1))\n        out = out.view(batch, channels, height, width)\n\n        return out + x  # Add the input x directly to the output of the self-attention\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:19.050265Z","iopub.execute_input":"2024-06-14T16:39:19.051083Z","iopub.status.idle":"2024-06-14T16:39:19.059738Z","shell.execute_reply.started":"2024-06-14T16:39:19.051050Z","shell.execute_reply":"2024-06-14T16:39:19.058861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.attention1 = SelfAttention(in_features)\n        \n        self.conv2 = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.attention2 = SelfAttention(in_features)\n        \n        self.conv3 = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features)\n        )\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.attention1(out)\n        out = self.conv2(out)\n        out = self.attention2(out)\n        out = self.conv3(out)\n        return x + out  # Skip connection from input to output","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:23.310108Z","iopub.execute_input":"2024-06-14T16:39:23.310798Z","iopub.status.idle":"2024-06-14T16:39:23.318767Z","shell.execute_reply.started":"2024-06-14T16:39:23.310766Z","shell.execute_reply":"2024-06-14T16:39:23.317872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeneratorResNet(nn.Module):\n    def __init__(self, input_shape, n_residual_blocks=9):\n        super(GeneratorResNet, self).__init__()\n        channels, img_height, img_width = input_shape\n        \n        # Initial convolution block\n        model = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(channels, 64, 7),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n        ]\n\n        # Downsampling\n        in_features = 64\n        out_features = in_features*2\n        for _ in range(2):\n            model += [\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n            out_features = in_features*2\n\n        # Residual blocks\n        for _ in range(n_residual_blocks):\n            model.append(ResidualBlock(in_features))\n\n        # Upsampling\n        out_features = in_features // 2\n        for _ in range(2):\n            model += [\n                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n            out_features = in_features // 2\n\n        # Output layer\n        model += [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(64, channels, 7),\n            nn.Tanh()\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:41.694066Z","iopub.execute_input":"2024-06-14T16:39:41.694851Z","iopub.status.idle":"2024-06-14T16:39:41.705366Z","shell.execute_reply.started":"2024-06-14T16:39:41.694817Z","shell.execute_reply":"2024-06-14T16:39:41.704356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n        channels, _, _ = input_shape\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:44.942770Z","iopub.execute_input":"2024-06-14T16:39:44.943105Z","iopub.status.idle":"2024-06-14T16:39:44.951489Z","shell.execute_reply.started":"2024-06-14T16:39:44.943078Z","shell.execute_reply":"2024-06-14T16:39:44.950560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nchannels, img_height, img_width = 3, 256, 256\ninput_shape = (channels, img_height, img_width)\nn_residual_blocks = 9\n\nG_AB = GeneratorResNet(input_shape, n_residual_blocks)\nG_BA = GeneratorResNet(input_shape, n_residual_blocks)\nD_A = Discriminator(input_shape)\nD_B = Discriminator(input_shape)\n\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_cycle = torch.nn.L1Loss()\ncriterion_identity = torch.nn.L1Loss()\n\noptimizer_G = Adam(list(G_AB.parameters()) + list(G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D_A = Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D_B = Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:48.393975Z","iopub.execute_input":"2024-06-14T16:39:48.394795Z","iopub.status.idle":"2024-06-14T16:39:48.873012Z","shell.execute_reply.started":"2024-06-14T16:39:48.394762Z","shell.execute_reply":"2024-06-14T16:39:48.872105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cuda = torch.cuda.is_available()\n\nif cuda:\n    G_AB = G_AB.cuda()\n    G_BA = G_BA.cuda()\n    D_A = D_A.cuda()\n    D_B = D_B.cuda()\n    \n    criterion_GAN.cuda()\n    criterion_cycle.cuda()\n    criterion_identity.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:55.128227Z","iopub.execute_input":"2024-06-14T16:39:55.128573Z","iopub.status.idle":"2024-06-14T16:39:55.423506Z","shell.execute_reply.started":"2024-06-14T16:39:55.128547Z","shell.execute_reply":"2024-06-14T16:39:55.422734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:39:59.591734Z","iopub.execute_input":"2024-06-14T16:39:59.592608Z","iopub.status.idle":"2024-06-14T16:39:59.600065Z","shell.execute_reply.started":"2024-06-14T16:39:59.592542Z","shell.execute_reply":"2024-06-14T16:39:59.599030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G_AB.apply(weights_init_normal)\nG_BA.apply(weights_init_normal)\nD_A.apply(weights_init_normal)\nD_B.apply(weights_init_normal)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:40:01.389807Z","iopub.execute_input":"2024-06-14T16:40:01.390464Z","iopub.status.idle":"2024-06-14T16:40:01.414731Z","shell.execute_reply.started":"2024-06-14T16:40:01.390432Z","shell.execute_reply":"2024-06-14T16:40:01.413897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LambdaLR:\n    def __init__(self, n_epochs, offset, decay_start_epoch):\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n    \n    def get_lr_lambda(self):\n        \"\"\"Returns a lambda function for the learning rate scheduler.\"\"\"\n        return lambda epoch: 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:40:11.770923Z","iopub.execute_input":"2024-06-14T16:40:11.771261Z","iopub.status.idle":"2024-06-14T16:40:11.776682Z","shell.execute_reply.started":"2024-06-14T16:40:11.771233Z","shell.execute_reply":"2024-06-14T16:40:11.775795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 15\nepoch = 0\ndecay_epoch = 5\n\n\n# Assuming optimizer_G is already defined\nlr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n    optimizer_G,\n    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).get_lr_lambda()\n)\n\nlr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n    optimizer_D_A,\n    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).get_lr_lambda()\n)\nlr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n    optimizer_D_B,\n    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).get_lr_lambda()\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:43:16.071080Z","iopub.execute_input":"2024-06-14T16:43:16.071924Z","iopub.status.idle":"2024-06-14T16:43:16.080778Z","shell.execute_reply.started":"2024-06-14T16:43:16.071881Z","shell.execute_reply":"2024-06-14T16:43:16.079632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_cycle_gan(dataloader, G_AB, G_BA, D_A, D_B, optimizer_G, optimizer_D_A, optimizer_D_B, criterion_GAN, criterion_cycle, criterion_identity, n_epochs=100, device='cuda'):\n    lambda_cycle = 10.0  # Weight for cycle-consistency loss\n    lambda_identity = 0.5 * lambda_cycle  # Weight for identity loss, often half of lambda_cycle\n\n    for epoch in range(n_epochs):\n        for i, batch in enumerate(dataloader):\n            real_A = batch['image_A'].to(device)\n            real_B = batch['image_B'].to(device)\n\n            valid = torch.ones(real_A.size(0), 1, requires_grad=False).to(device)\n            fake = torch.zeros(real_A.size(0), 1, requires_grad=False).to(device)\n\n            optimizer_G.zero_grad()\n\n            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n            loss_identity = (loss_id_A + loss_id_B) / 2\n\n            fake_B = G_AB(real_A)\n            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n            fake_A = G_BA(real_B)\n            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n\n            # Cycle loss\n            recov_A = G_BA(fake_B)\n            loss_cycle_A = criterion_cycle(recov_A, real_A)\n            recov_B = G_AB(fake_A)\n            loss_cycle_B = criterion_cycle(recov_B, real_B)\n            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n            total_loss_G = loss_GAN + lambda_cycle * loss_cycle + lambda_identity * loss_identity\n            total_loss_G.backward()\n            optimizer_G.step()\n            optimizer_D_A.zero_grad()\n\n            loss_real_A = criterion_GAN(D_A(real_A), valid)\n            loss_fake_A = criterion_GAN(D_A(fake_A.detach()), fake)\n            total_loss_D_A = (loss_real_A + loss_fake_A) / 2\n            total_loss_D_A.backward()\n            optimizer_D_A.step()\n\n            optimizer_D_B.zero_grad()\n\n            loss_real_B = criterion_GAN(D_B(real_B), valid)\n            loss_fake_B = criterion_GAN(D_B(fake_B.detach()), fake)\n            total_loss_D_B = (loss_real_B + loss_fake_B) / 2\n            total_loss_D_B.backward()\n            optimizer_D_B.step()\n            print(f'[Epoch {epoch+1}/{n_epochs}] [Batch {i+1}/{len(dataloader)}] [D loss: {total_loss_D_A.item() + total_loss_D_B.item():.6f}] [G loss: {total_loss_G.item():.6f} - (adv: {loss_GAN.item():.6f}, cycle: {lambda_cycle * loss_cycle.item():.6f}, identity: {lambda_identity * loss_identity.item():.6f})]')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:43:17.362373Z","iopub.execute_input":"2024-06-14T16:43:17.363011Z","iopub.status.idle":"2024-06-14T16:43:17.378008Z","shell.execute_reply.started":"2024-06-14T16:43:17.362981Z","shell.execute_reply":"2024-06-14T16:43:17.376910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cycle_gan(\n    dataloader=train_dataloader,\n    G_AB=G_AB,\n    G_BA=G_BA,\n    D_A=D_A,\n    D_B=D_B,\n    optimizer_G=optimizer_G,\n    optimizer_D_A=optimizer_D_A,\n    optimizer_D_B=optimizer_D_B,\n    criterion_GAN=criterion_GAN,\n    criterion_cycle=criterion_cycle,\n    criterion_identity=criterion_identity,\n    n_epochs=n_epochs,\n    device=device\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T16:43:19.641394Z","iopub.execute_input":"2024-06-14T16:43:19.641786Z","iopub.status.idle":"2024-06-14T19:13:24.181991Z","shell.execute_reply.started":"2024-06-14T16:43:19.641755Z","shell.execute_reply":"2024-06-14T19:13:24.180848Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, batch in enumerate(val_dataloader):\n    image_A = batch['image_A'].to(device)\n    image_B = batch['image_B'].to(device)\n    print(f'iter : {i}  image_A.size : {image_A.size()}')\n    print(f'iter : {i}  image_B.size : {image_B.size()}')\n\n    if i == 10:\n        break\n        \nphoto_dir = os.path.join(data_dir, 'photo_jpg')\nfiles = [os.path.join(photo_dir, name) for name in os.listdir(photo_dir)]\nlen(files)\n\nsave_dir = '/kaggle/working/img/'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n    \nfor file in os.listdir(save_dir):\n    os.remove(os.path.join(save_dir, file))\n    \n    \nbatch_size = 1\n\ngenerate_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nto_image = transforms.ToPILImage()\n\nG_BA.eval()\nfor i in range(0, len(files), batch_size):\n    # read images\n    imgs = []\n    for j in range(i, min(len(files), i + batch_size)):\n        try:\n            img = Image.open(files[j])\n            img = generate_transforms(img)\n            imgs.append(img)\n        except Exception as e:\n              print(f\"Error processing {files[j]}: {e}\")\n    \n    \n    imgs = torch.stack(imgs, 0).to(device) \n\n    # generate\n    fake_imgs = G_BA(imgs).detach().cpu()\n\n\n# save\n    for j in range(fake_imgs.size(0)):\n        img = fake_imgs[j].squeeze().permute(1, 2, 0)\n        img_arr = img.numpy()\n        img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n        img_arr = img_arr.astype(np.uint8)\n\n        img = to_image(img_arr)\n        _, name = os.path.split(files[i+j])\n        img.save(os.path.join(save_dir, name))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:38:21.447503Z","iopub.execute_input":"2024-06-14T19:38:21.447884Z","iopub.status.idle":"2024-06-14T19:52:27.258093Z","shell.execute_reply.started":"2024-06-14T19:38:21.447852Z","shell.execute_reply":"2024-06-14T19:52:27.256986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}